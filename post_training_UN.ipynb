{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Testing the trained network and output generation\"\n",
    "author:\n",
    "  - name: Adriano Fossati\n",
    "    url: https://github.com/AdrianoGuidoF\n",
    "    orcid: 0009-0008-9751-4599\n",
    "    affiliation: Queensland University of Technology\n",
    "    email: \"afossati.academia@gmail.com\"\n",
    "date: today\n",
    "format:\n",
    "    html:\n",
    "        toc: true\n",
    "        number_sections: true\n",
    "        code-fold: true # to hide the code by default, but have the option to show it\n",
    "        # code-fold: show # to show the code by default, but have the option to hide it\n",
    "        code-tools: true\n",
    "        code-summary: \"Show the code\"\n",
    "        code-overflow: scroll\n",
    "        # embed-resources: true\n",
    "        css: styles.css\n",
    "# bibliography: references.bib\n",
    "abstract: |\n",
    "  Testing of the YOLOv5 neural network model that has been trained. \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will guide you through the steps to test and evaluate the accuracy of the trained model. More specifically, it prepares the padded png image for manual labelling, segments the labelled image, runs the detection on the same non-labelled image and backward annotates it, compares your manual annotation to the detected annotation, and renders a confusion matrix to evaluate the model performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Paths definition and set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fossatia\\AppData\\Local\\miniconda3\\envs\\Boats\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "YOLOv5  v7.0-394-g86fd1ab2 Python-3.10.16 torch-1.12.1+cu113 CUDA:0 (GeForce GTX 1080, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import yaml \n",
    "import argparse\n",
    "import os.path as path\n",
    "import scipy.cluster\n",
    "import scipy.spatial\n",
    "import json\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import stat\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "from counting_wh.wh_utils.config import cfg\n",
    "from counting_wh.wh_utils import image_cutting_support as ics\n",
    "from counting_wh.wh_utils import heatmap as hm\n",
    "\n",
    "import counting_wh.wh_utils.classifier\n",
    "\n",
    "# Add the project root to sys.path (adjust as needed)\n",
    "sys.path.append(os.path.abspath(\"Waterholes_project/WaterholeDetection_UN-Handbook\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Preparation of the padded PNG image\n",
    "Directly from the tif file, this step prepares the padded png image for future steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import counting_wh.wh_utils.testing\n",
    "\n",
    "counting_wh.wh_utils.testing.prepare(\"Waterholes_project/WaterholeDetection_UN-Handbook/testing\", \"config_test_Drive_UN.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Manual annotation \n",
    "Annotate manually the created png with labelme as in training step. Refer to the previous tab for detailed instructions on how to use labelme. Make sure to use the same categories as the training step. This will allow us to compare my annotation to the detection of the trained model i.e. test the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Segmentation of the annotated image\n",
    "Once the manual annotation is done, we can apply the segmentation. Note that this segmentation doesn't split 20% of the images for the model validation, meaning all segmented images will be used for the testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping Image: D:/Waterholes_project/counting_waterholes/testing\\./pngs\\20240204_mimal_test.png\n",
      "[12064 14976     3]\n",
      "We will have:  15933  images maximum\n",
      "0% of images without labels will be removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Segments: 100%|██████████| 15933/15933 [05:33<00:00, 47.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 728 images\n",
      "Empty 0 images\n",
      "Cropping Image: D:/Waterholes_project/counting_waterholes/testing\\./pngs\\20240324_mimal_test.png\n",
      "[12064 14976     3]\n",
      "We will have:  15933  images maximum\n",
      "0% of images without labels will be removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Segments: 100%|██████████| 15933/15933 [08:10<00:00, 32.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 1676 images\n",
      "Empty 0 images\n",
      "Cropping Image: D:/Waterholes_project/counting_waterholes/testing\\./pngs\\20240415_mimal_test.png\n",
      "[12064 14976     3]\n",
      "We will have:  15933  images maximum\n",
      "0% of images without labels will be removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Segments: 100%|██████████| 15933/15933 [11:48<00:00, 22.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 790 images\n",
      "Empty 0 images\n",
      "Segregating by day...\n",
      "01_01_2024\n",
      "04_02_2024\n",
      "24_03_2024\n",
      "15_04_2024\n",
      "Segregating by image...\n",
      "Segregating by image...\n",
      "Segregating by image...\n",
      "Segregating by image...\n",
      "Segregating by day...\n",
      "01_01_2024\n",
      "04_02_2024\n",
      "24_03_2024\n",
      "15_04_2024\n",
      "Segregating by image...\n",
      "Segregating by image...\n",
      "Segregating by image...\n",
      "Segregating by image...\n"
     ]
    }
   ],
   "source": [
    "#run segmentation without spliting 80% of the images for validation!\n",
    "import counting_wh.wh_utils.testing\n",
    "\n",
    "counting_wh.wh_utils.testing.segment(r\"Waterholes_project/WaterholeDetection_UN-Handbook/testing\", \"config_test_Drive_UN.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Waterholes detection with the trained model\n",
    "Using those segmented labelled images, we can run the detection of waterholes using the trained model, and compare my manual label with the detection of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging section to recognise and work well with the GPU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to user: If you are running on a GPU, you need to update the torchvision to match the cuda (GPU) version. Using the 'nvidia-smi' command, you get your cuda version (in my case: 11) so I need to get a version of torch and torchaudio as 11.xx.  \n",
    "Need to 'pip uninstall torch torchvision', and then install the correct version, in my case: 'pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113'  \n",
    "Other version to be found on this website: https://pytorch.org/get-started/previous-versions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing.segment function and run_detection work to use the segmented images folders grouped per date. Left as it is for now but just something to bear in mind!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check of the GPU found or not?\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPU is found, so we can proceed with the detection: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run detection on my testing image set:\n",
    "import counting_wh.wh_utils.testing\n",
    "\n",
    "counting_wh.wh_utils.testing.run_detection(r\"C:/Users/adria/OneDrive - AdrianoFossati/Documents/MASTER Australia/RA/Waterholes_project/WaterholeDetection_UN-Handbook/testing\", \"config_test_Drive_UN.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy of the order command that is run run to yolo cmd automatically by the function run_detection. Just for you to check and have the information in order to debug or adapt to your computational power. Do not run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python C:/Users/adria/OneDrive - AdrianoFossati/Documents/MASTER Australia/RA/Waterholes_project/yolov5/detect.py --imgsz 416 --save-txt --save-conf --weights C:/Users/adria/OneDrive - AdrianoFossati/Documents/MASTER Australia/RA/Waterholes_project/yolov5/runs/train/exp3/weights/best.pt --source D:\\\\Waterholes_project\\\\WaterholeDetection_UN-Handbook\\\\testing\\\\segmented_images\\\\04_02_2024\\\\20240204_mimal_test --device cuda:0 --nosave --conf-thres 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Backward annotation \n",
    "We can now use the detection output of the model on my testing images to produce labelme style annotation.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import counting_wh.wh_utils.testing\n",
    "\n",
    "counting_wh.wh_utils.testing.backwards_annotation_AF(r\"C:/Users/adria/OneDrive - AdrianoFossati/Documents/MASTER Australia/RA/Waterholes_project/WaterholeDetection_UN-Handbook/testing\", \"config_test_Drive_UN.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Ground truth comparison\n",
    "Then finally, we can compare the model-detected waterholes with the ones you manually labeled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label directory D:/Waterholes_project/counting_waterholes/testing_v3\\./labels\\05_02_2025\\050225_NT does not exist, skipping image...\n",
      "raw_images folder D:/Waterholes_project/counting_waterholes/testing_v3\\./raw_images\n",
      "Could not parse date from 050225_NT.csv\n"
     ]
    }
   ],
   "source": [
    "import counting_wh.wh_utils.testing\n",
    "\n",
    "counting_wh.wh_utils.testing.compare_detections_to_ground_truth(r\"C:/Users/adria/OneDrive - AdrianoFossati/Documents/MASTER Australia/RA/Waterholes_project/WaterholeDetection_UN-Handbook/testing\", \"config_test_Drive_UN.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Confusion matrix\n",
    "Create the confusion matrix which summaries the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import counting_wh.wh_utils.testing\n",
    "\n",
    "counting_wh.wh_utils.testing.confusion_matrix_AF(r\"C:/Users/adria/OneDrive - AdrianoFossati/Documents/MASTER Australia/RA/Waterholes_project/WaterholeDetection_UN-Handbook/testing\", \"config_test_Drive_UN.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Extra testing outputs \n",
    "I. Possible to process a single images by comparing the detections and labels for a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import counting_wh.wh_utils.testing\n",
    "\n",
    "counting_wh.wh_utils.testing.process_image_AF(r\"C:/Users/adria/OneDrive - AdrianoFossati/Documents/MASTER Australia/RA/Waterholes_project/WaterholeDetection_UN-Handbook/testing_v3\\classifications\", r\"C:/Users/adria/OneDrive - AdrianoFossati/Documents/MASTER Australia/RA/Waterholes_project/WaterholeDetection_UN-Handbook/testing_v3/labels\", \"config_test_Drive_UN.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Possible to compare the result of single waterhole detection versus annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import counting_wh.wh_utils.testing\n",
    "\n",
    "counting_wh.wh_utils.testing.waterholes_count_compare(r\"C:/Users/adria/OneDrive - AdrianoFossati/Documents/MASTER Australia/RA/Waterholes_project/WaterholeDetection_UN-Handbook/testing_v3\", \"config_test_Drive_UN.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Global image generation\n",
    "Now we want to plot the waterholes on the images. To do that, I need first to stich the training images together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max x: 92, Max y: 60\n",
      "Saved stitched image to D:\\Waterholes_project\\counting_waterholes\\testing_v3\\stitching\\stitched.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "from PIL import Image\n",
    "import counting_wh.wh_utils.stitch_PNGs\n",
    "\n",
    "counting_wh.wh_utils.stitch_PNGs.stitch(r\"C:/Users/adria/OneDrive - AdrianoFossati/Documents/MASTER Australia/RA/Waterholes_project/WaterholeDetection_UN-Handbook\\testing_v3\\stitching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to plot the waterholes, but the function is made for boats so I need to modify it to work on WH. WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot WH using that stitched image:\n",
    "import counting_wh.wh_utils.testing\n",
    "\n",
    "counting_wh.wh_utils.testing.plot_waterholes(r\"C:/Users/adria/OneDrive - AdrianoFossati/Documents/MASTER Australia/RA/Waterholes_project/WaterholeDetection_UN-Handbook/testing_v3\", \"config_test_Drive_UN.yaml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Boats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
