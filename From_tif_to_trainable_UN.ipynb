{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Training preparation steps\"\n",
    "author:\n",
    "  - name: Adriano Fossati\n",
    "    url: https://github.com/AdrianoGuidoF\n",
    "    orcid: 0009-0008-9751-4599\n",
    "    affiliation: Queensland University of Technology\n",
    "    email: \"afossati.academia@gmail.com\"\n",
    "date: today\n",
    "format:\n",
    "    html:\n",
    "        toc: true\n",
    "        number_sections: true\n",
    "        code-fold: true # to hide the code by default, but have the option to show it\n",
    "        # code-fold: show # to show the code by default, but have the option to hide it\n",
    "        code-tools: true\n",
    "        code-summary: \"Show the code\"\n",
    "        code-overflow: scroll\n",
    "        # embed-resources: true\n",
    "        css: styles.css\n",
    "# bibliography: references.bib\n",
    "abstract: |\n",
    "  This script handles the image zip file once they have been downloaded using the planet_download.ipynb Jupyter notebook up to a product allowing us to train a model.  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  \n",
    "This script extracts the image from the .zip file downloaded from Planet; prepares the images creating the .png image from the .tif file allowing for the manual annotation wit LabelMe; segments the labelled image; describe that segmented image for an evaluation of their relevance to train the model; culls them to suit the model training requirements; and finally apply the model training. \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. \n",
    "We define the path of the project, and refer the correct python scripts containing the functions that are automatically called when running dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "# Add the project root to sys.path (adjust as needed)\n",
    "sys.path.append(os.path.abspath(\"Waterholes_project/WaterholeDetection_UN-Handbook\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. \n",
    "If we were to follow exactly the procedure and download the zip file from Planet, we would need to use the extract_zip function as demostrated in the following code section. It would extract the file called \"composite.tif\" obtained from the planet order and downloaded into the zip file in our \"raw_images\" folder. It will render a tif file and will rename it with the date_aoi.tif outside the zip file.   \n",
    "However, for this tutorial, we provide in the link from the \"Getting started\" section to download the example image. This image is in the .tif format and will be manipulated further in the section nÂ°3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import counting_wh.wh_utils.planet_utils\n",
    "\n",
    "# Define the path to your zip file\n",
    "zip_path = \"images/raw_images\" \n",
    "#AFUN: should this path be training/raw_images? \n",
    "#If so, need to be careful where we download the image zip file. Might need to adaot this. \n",
    "\n",
    "# Run extraction\n",
    "counting_wh.wh_utils.planet_utils.extract_zip(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. \n",
    "We will now transform the example raw tif image into a usable png for future steps. It creates a padded png image to exactly match the size dividable by the stride and tile size.   \n",
    "Caution that you need to define the config file called \"config_train_Drive_UN\" as instructed to make sure it matches your paths and runs everything smoothly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing gdal work...\n",
      "Done with gdal work for 20240101_mimal_test.tif\n",
      "New Width:  14976 New Height:  12064\n",
      "Processed 1/1 images\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: show\n",
    " \n",
    "import os\n",
    "import yaml\n",
    "import counting_wh.train\n",
    "\n",
    "#Run preparation of the tif files into png and renamed the tif. \n",
    "counting_wh.train.prepare(\"config_train_Drive_UN.yaml\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. \n",
    "Once the .png image is created, as we are training the model, we need to label the training image. In order to do so, you need to use LabelMe.  \n",
    "LabelMe is called in your terminal, manually typing \"labelme\".   \n",
    "XXXXXXX insert image of label me opening, to make sure they can open it via the terminal? \n",
    "\n",
    "You have to then annotate the waterholes on your padded png image. This creates at the end a .json file with all my bounding boxes definitions. Those labels will be needed in the next step to segment the image and the corresponding labels for training purposes. \n",
    "XXXXXXXX insert gif of manual label me annotation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \n",
    "\n",
    "Once the whole manual annotation is done, save the outputs, and come back to this script to run the segmentation of the padded png image you just labelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import counting_wh.train\n",
    "\n",
    "#segment the png images\n",
    "counting_wh.train.segment(\"config_train_Drive_UN.yaml\", train_val_split=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. \n",
    "After the segmentation, we evaluate the result of the segmentation and production of material to train the model using the \"train.describe\" function. \n",
    "Run the bellow cell to describe the results of segmented images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config path: D:\\Waterholes_project\\counting_waterholes\\training_v4\\output\\images\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcounting_boats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#describe the created segmented images: \u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mcounting_boats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescribe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig_train_Drive.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fossatia\\Documents\\Waterholes_project\\counting_waterholes\\counting_boats\\train.py:216\u001b[0m, in \u001b[0;36mdescribe\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    213\u001b[0m lab_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(labdir, lab)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# Check if file is empty first (file size = 0)\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlab_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    217\u001b[0m     num_tiles_no_labels \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fossatia\\AppData\\Local\\miniconda3\\envs\\Boats\\lib\\genericpath.py:50\u001b[0m, in \u001b[0;36mgetsize\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetsize\u001b[39m(filename):\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the size of a file, reported by os.stat().\"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mst_size\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import counting_wh.train\n",
    "\n",
    "#describe the created segmented images: \n",
    "counting_wh.train.describe(\"config_train_Drive_UN.yaml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. \n",
    "Before proceeding to the training of the model, we need to apply the cull command which will remove images with no labels until 10% of the training set has no labels. This is a recommended procedure from Yolo to maintain efficient model training parameters. This has to be done post segmentation as we don't know prior the the amount.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing label files in: D:\\Waterholes_project\\counting_waterholes\\training_v4\\output\\labels\\train\n",
      "Looking for corresponding images in: D:\\Waterholes_project\\counting_waterholes\\training_v4\\output\\images\\train\n",
      "Total label files found: 13565\n",
      "Empty label files found: 6063\n",
      "Non-empty label files: 7502\n",
      "Moving 5230 empty label files to maintain 10% ratio\n",
      "\n",
      "--- SUMMARY ---\n",
      "Total label files moved: 5230\n",
      "Total image files moved: 5230\n",
      "Remaining total label files: 8335\n",
      "Remaining empty label files: 833\n",
      "Empty labels now make up 9.99% of the dataset\n",
      "Empty labels moved to: D:\\Waterholes_project\\counting_waterholes\\training_v4\\output\\labels\\moved_empty_labels\n",
      "Corresponding images moved to: D:\\Waterholes_project\\counting_waterholes\\training_v4\\output\\images\\moved_empty_images\n",
      "\n",
      "SUCCESS: Empty labels now make up 10% or less of the dataset.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path \n",
    "import counting_wh.train\n",
    "\n",
    "#describe the created segmented images: \n",
    "counting_wh.train.cull_AF(\"config_train_Drive_UN.yaml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. \n",
    "Now that we have less than 10% of the images unlabelled, we can train the model, but first let's reorganise the folders to be properly used in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying from D:\\Waterholes_project\\counting_waterholes\\training_v4\\output\\images\\val to D:\\Waterholes_project\\counting_waterholes\\training_v4\\train\\val\\images\n",
      "Successfully copied to D:\\Waterholes_project\\counting_waterholes\\training_v4\\train\\val\\images\n",
      "Copying from D:\\Waterholes_project\\counting_waterholes\\training_v4\\output\\images\\train to D:\\Waterholes_project\\counting_waterholes\\training_v4\\train\\train\\images\n",
      "Successfully copied to D:\\Waterholes_project\\counting_waterholes\\training_v4\\train\\train\\images\n",
      "Copying from D:\\Waterholes_project\\counting_waterholes\\training_v4\\output\\labels\\val to D:\\Waterholes_project\\counting_waterholes\\training_v4\\train\\val\\labels\n",
      "Successfully copied to D:\\Waterholes_project\\counting_waterholes\\training_v4\\train\\val\\labels\n",
      "Copying from D:\\Waterholes_project\\counting_waterholes\\training_v4\\output\\labels\\train to D:\\Waterholes_project\\counting_waterholes\\training_v4\\train\\train\\labels\n",
      "Successfully copied to D:\\Waterholes_project\\counting_waterholes\\training_v4\\train\\train\\labels\n",
      "\n",
      "----- REORGANIZATION SUMMARY -----\n",
      "Successful operations: 4\n",
      "Failed operations: 0\n",
      "\n",
      "All folders were successfully reorganized!\n",
      "\n",
      "New structure:\n",
      "- D:\\Waterholes_project\\counting_waterholes\\training_v4\\train/val/images (copied from D:\\Waterholes_project\\counting_waterholes\\training_v4\\output/images/val)\n",
      "- D:\\Waterholes_project\\counting_waterholes\\training_v4\\train/val/labels (copied from D:\\Waterholes_project\\counting_waterholes\\training_v4\\output/labels/val)\n",
      "- D:\\Waterholes_project\\counting_waterholes\\training_v4\\train/train/images (copied from D:\\Waterholes_project\\counting_waterholes\\training_v4\\output/images/train)\n",
      "- D:\\Waterholes_project\\counting_waterholes\\training_v4\\train/train/labels (copied from D:\\Waterholes_project\\counting_waterholes\\training_v4\\output/labels/train)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import counting_wh.train\n",
    "\n",
    "counting_wh.train.reorganize_folders(\"config_train_Drive_UN.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. \n",
    "The following code provides you with the line of code to run in yolo in order to train a neural network model. Before doing so, modifiy the directory of the yolo model training path in the config file called \"config_train_Drive_UN.yaml\". \n",
    "\n",
    "Then, run this code which provides you with the command to excecute in the cmd terminal of the Yolov5.  \n",
    "XXXXXXXXXXXXXXX insert screenshot of the yoloy terminal and what it looks like when it's training? Maybe a screen shot and a gif when it trains?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mpython C:/Users/fossatia/Documents/Waterholes_project/yolov5/train.py --device cuda:0 --img 416 --batch 8 --workers 6 --epochs 100 --data config_train_Drive.yaml --weights C:/Users/fossatia/Documents/Waterholes_project/yolov5/runs/train/exp3/weights/best.pt --save-period 50\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import counting_wh.train\n",
    "\n",
    "#describe the created segmented images: \n",
    "counting_wh.train.train(\"config_train_Drive_UN.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment from AF (08.09): Need to run it and see what it actually prints. Because I ran with the line bellow to train. So need to make sure what we advise to the user as well in term of batch size, img, workers epochs etc... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train.py --workers 2 --img 416 --batch 8 --epochs 150 --data config_train_Drive_UN.yaml --weights yolov5s.pt --cache disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it actually seems that it runs automatically with the function train but we do not see any progression... \n",
    "Prefer for now to run it manually in the cmd of the yolov5 folder.  \n",
    "\n",
    "Changed the cache to the SSD drive we are using as we are limited in the storage available locally. \n",
    "Needed to set the project to the SSD which saves the outputs and doesn't increase the C: storage usage. \n",
    "\n",
    "Need to actually create the D:/temp and D:/yolo_run on your Drive or external directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set TMPDIR=D:/temp\n",
    "set TEMP=D:/temp\n",
    "set TMP=D:/temp\n",
    "set KMP_DUPLICATE_LIB_OK=TRUE \n",
    "python C:/Users/fossatia/Documents/Waterholes_project/yolov5/train.py --device cuda:0 --img 416 --batch 4 --workers 2 --epochs 50 --data C:\\Users\\fossatia\\Documents\\Waterholes_project\\counting_waterholes\\config_train_Drive.yaml --weights C:/Users/fossatia/Documents/Waterholes_project/yolov5/runs/train/exp3/weights/best.pt --cache False --project D:/yolo_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set KMP_DUPLICATE_LIB_OK=TRUE is not recommended on the error command... I tried to google it and it seems we should force an install of the Nomkl using 'conda install nomkl --channel conda-forge'. \n",
    "However, by doing so, dependencies might be altered. To be checked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of this script. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Boats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
