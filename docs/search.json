[
  {
    "objectID": "S2_download_GEE.html",
    "href": "S2_download_GEE.html",
    "title": "GEE Environmental Covariates",
    "section": "",
    "text": "Show the code\nimport os\nimport ee\nimport geemap\nimport json\nimport requests\n\nfrom PIL import Image\n\n\n\n\nShow the code\n# Initialize Earth Engine\ntry:\n    ee.Initialize()\n    print(\"Earth Engine already initialized\")\nexcept Exception as e:\n    ee.Authenticate()\n    ee.Initialize()\n    print(\"Earth Engine initialized\")\n\n\n\n            \n            \n\n\nEarth Engine already initialized\n\n\n\n\n\n\nShow the code\nMap = geemap.Map(lite_mode=True)\nMap.add_basemap(\"SATELLITE\")\nMap\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\n\nShow the code\n# Option 1: Read JSON from file\ndef load_aoi_from_file(json_file_path):\n    with open(json_file_path, 'r') as f:\n        geojson = json.load(f)\n    \n    # Create an ee.Geometry from the GeoJSON\n    return ee.Geometry(geojson)\n\n\n\n            \n            \n\n\n\n\nShow the code\n# Option 1: Define AOI as a geometry\n# aoi = ee.Geometry.Rectangle([-122.5, 37.5, -122.0, 38.0])  # San Francisco area\n\naoi_name = 'example'\naoi = load_aoi_from_file(fr'C:\\Users\\for329\\OneDrive - Queensland University of Technology\\FirstByte Waterholes WD\\counting_waterholes\\data\\polygons\\{aoi_name}.geojson')\n\n# Show the AOI on map\nMap.addLayer(aoi, {}, 'Area of Interest')\n# Center the map on the AOI and zoom to it\nMap.centerObject(aoi, zoom=10)  # Adjust zoom level (1-20) as needed\nMap  # Display the map with the AOI"
  },
  {
    "objectID": "S2_download_GEE.html#plot-the-basemap",
    "href": "S2_download_GEE.html#plot-the-basemap",
    "title": "GEE Environmental Covariates",
    "section": "",
    "text": "Show the code\nMap = geemap.Map(lite_mode=True)\nMap.add_basemap(\"SATELLITE\")\nMap"
  },
  {
    "objectID": "S2_download_GEE.html#import-aoi",
    "href": "S2_download_GEE.html#import-aoi",
    "title": "GEE Environmental Covariates",
    "section": "",
    "text": "Show the code\n# Option 1: Read JSON from file\ndef load_aoi_from_file(json_file_path):\n    with open(json_file_path, 'r') as f:\n        geojson = json.load(f)\n    \n    # Create an ee.Geometry from the GeoJSON\n    return ee.Geometry(geojson)\n\n\n\n            \n            \n\n\n\n\nShow the code\n# Option 1: Define AOI as a geometry\n# aoi = ee.Geometry.Rectangle([-122.5, 37.5, -122.0, 38.0])  # San Francisco area\n\naoi_name = 'example'\naoi = load_aoi_from_file(fr'C:\\Users\\for329\\OneDrive - Queensland University of Technology\\FirstByte Waterholes WD\\counting_waterholes\\data\\polygons\\{aoi_name}.geojson')\n\n# Show the AOI on map\nMap.addLayer(aoi, {}, 'Area of Interest')\n# Center the map on the AOI and zoom to it\nMap.centerObject(aoi, zoom=10)  # Adjust zoom level (1-20) as needed\nMap  # Display the map with the AOI"
  },
  {
    "objectID": "S2_download_GEE.html#get-monthly-composites-of-sentinel-2",
    "href": "S2_download_GEE.html#get-monthly-composites-of-sentinel-2",
    "title": "GEE Environmental Covariates",
    "section": "Get Monthly Composites of Sentinel-2",
    "text": "Get Monthly Composites of Sentinel-2\n\n\nShow the code\ndef get_monthly_composites(start_date, end_date, aoi):\n    \"\"\"Generate monthly S2 composites for the given date range and AOI.\"\"\"\n    # Convert date strings to ee.Date objects\n    start = ee.Date(start_date)\n    end = ee.Date(end_date)\n    \n    # Get number of months\n    months = end.difference(start, 'month').round().int()\n\n    # First, check if we have any S2 data for this month (without filtering)\n    raw_collection = ee.ImageCollection('COPERNICUS/S2_SR') \\\n        .filterBounds(aoi) \\\n        .filterDate(start_date, end_date)\n    \n    raw_count = raw_collection.size().getInfo()\n    print(f\"  Found {raw_count} raw S2 images for {start_date} to {end_date}\")\n    \n    # Function to get image for a specific month\n    def get_monthly_image(month_index):\n        # Calculate current month start and end\n        current_month_start = start.advance(month_index, 'month')\n        current_month_end = current_month_start.advance(1, 'month')\n        \n        # Format dates for naming\n        date_format = current_month_start.format('YYYY-MM')\n        \n        # Get S2 collection for this month\n        s2_collection = ee.ImageCollection('COPERNICUS/S2_SR') \\\n            .filterBounds(aoi) \\\n            .filterDate(current_month_start, current_month_end) \\\n            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 90)) \\\n            .map(maskS2clouds_SCL)\n        \n        # # Check if we have any images left after cloud filtering\n        # filtered_count = s2_collection.size().getInfo()\n        # print(f\"  After cloud filtering: {filtered_count} images remain\")\n        \n        # Create a median composite and clip to AOI\n        composite = s2_collection.median().clip(aoi)\n        \n        # Select RGB bands and scale for visualization\n        # Bands: B2 (blue), B3 (green), B4 (red)\n        # These get re-ordered to RGB by the image_cutting_support.create_padded_png_S2 function\n        rgb = composite.select(['B2', 'B3', 'B4'])\n        \n        return rgb.set({\n            'system:index': date_format,\n            'system:time_start': current_month_start.millis()\n        })\n    \n    # Create a list of months\n    month_indices = ee.List.sequence(0, months)\n    \n    # Map the function over the months\n    composites = ee.ImageCollection.fromImages(\n        month_indices.map(get_monthly_image)\n    )\n    \n    return composites"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Waterhole Detection - UN Handbook",
    "section": "",
    "text": "This project aims to detect and assess the locations of waterholes in Northern Australia using satellite imagery and machine learning. The primary focus is on identifying waterholes susceptible to damage from invasive herbivores such as water buffalo, feral cattle, and pigs."
  },
  {
    "objectID": "index.html#project-background",
    "href": "index.html#project-background",
    "title": "Waterhole Detection - UN Handbook",
    "section": "Project Background",
    "text": "Project Background\nAdapted from an existing boat detection pipeline, this project leverages satellite image analysis and YOLOv5 object detection to map and evaluate waterholes in the Arnhem Land and Cape York regions. While originally based on the CountingBoats repository, our workflow has been modified to use Jupyter notebooks for increased clarity and control."
  },
  {
    "objectID": "index.html#how",
    "href": "index.html#how",
    "title": "Waterhole Detection - UN Handbook",
    "section": "How",
    "text": "How\nThe project follows a comprehensive pipeline:\n\nImage Acquisition:\n\n\nUse Planet satellite imagery service to order recent images of the target area\nAutomatically download imagery for the specified region and date range\n\n\nPre-processing:\n\n\nPrepare satellite images for neural network detection\nConvert and normalize imagery to suitable format\n\n\nDetection:\n\n\nUtilize a pre-trained YOLOv5 model to detect and classify waterholes\nGenerate comprehensive output with waterhole locations, classifications, and coordinates\n\n\nAnalysis:\n\n\nCollate detection results\nOutput CSV with detailed waterhole information"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Getting Started",
    "section": "",
    "text": "Installation\nClone the repositry here https://github.com/swforrest/WaterholeDetection_UN-Handbook. This will allow you to run all the notebooks and follow the instructions below.\n\nYolov5\nClone YoloV5. This is used for the Neural Network detections.\n\n\nPython Dependencies\nIt’s recommended to install a conda-based package manager such as Miniconda.\nRunning the following will then install all required dependencies (run only once to set the environment up):\nconda env create --file env.yaml\n\n\n\nSetup\nTo start working on this project, activate the environment with:\nconda activate Boats\n\nConfigurations\nModify configuration files to match your specific environment and research requirements: - config_train_Drive.yaml: Training configuration - config_test_Drive.yaml: Testing configuration\n- config_deploy_Drive.yaml: Deployment configuration\n\n\n\nRunning\nAs mentionned, the whole pipeline flow designed by Charlie Turner for the boat detection, is not used here. You will need to run specific blocks of code in the respective jupyter notebooks of each stage of our workflow.\n\nImages dowload\nUsing the notebook planet_download.ipynb, you will: - Define Area of Interest (AOI) - Specify output directory and date range - Download and extract satellite imagery\n\n\nPrepare for training\nUsing the notebook from_tif_to_trainable_AF.ipynb, you will: - Convert .tif files to training-ready format - Prepare data for neural network training\n\n\nTraining\nEither simply run the last code block from from_tif_to_trainable_AF.ipynb function train() or copy paste the function output in the yolov5 folder command prompt.\n\n\nTesting\nUsing the notebook post_training.ipynb, the notebook will guide you along the main steps to test the model you just trained. Those steps are:\n- Prepare images for segmentation - Run model detection - Generate annotations - Compare detections to ground truth - Create confusion matrix - Visualize waterhole detections\n\n\nDeployment\nOriginally the repository had a whole section about the classifying process, which included many functions dependent on the classes of the detection. The adaptation of this existing code to waterhole detection, has proven to be more difficult and time consumming than anticipated. As a result, The deployment section of the repository is still under development and requires further debugging. Some functions are created but need refinement for smooth operation.\n\n\nVisualisation\nThere are some visualisation notebooks in the visualisation folder. These can be run to perform some visualisations of the data. The plot_output script is also a useful tool for visualising the output of the detection model on individual images.\nAll thos functions and modules were developped for the visualisation of Boats and was not adapted to waterholes by lack of time.\n\n\n\nAcknowledgements\nWe acknowledge Charlie Turner and other collaborators for their work on the Boat detection repository, largely foundamental to the existance of this repository."
  },
  {
    "objectID": "From_tif_to_trainable_UN.html",
    "href": "From_tif_to_trainable_UN.html",
    "title": "Training preparation steps",
    "section": "",
    "text": "This script extracts the image from the .zip file downloaded from Planet; prepares the images creating the .png image from the .tif file allowing for the manual annotation wit LabelMe; segments the labelled image; describe that segmented image for an evaluation of their relevance to train the model; culls them to suit the model training requirements; and finally apply the model training.\n\n1.\nWe define the path of the project, and refer the correct python scripts containing the functions that are automatically called when running dependencies.\n\n\nShow the code\nimport sys\nimport os\nimport yaml\n\n# Add the project root to sys.path (adjust as needed)\nsys.path.append(os.path.abspath(\"Waterholes_project/WaterholeDetection_UN-Handbook\"))\n\n\n\n\n2.\nIf we were to follow exactly the procedure and download the zip file from Planet, we would need to use the extract_zip function as demostrated in the following code section. It would extract the file called “composite.tif” obtained from the planet order and downloaded into the zip file in our “raw_images” folder. It will render a tif file and will rename it with the date_aoi.tif outside the zip file.\nHowever, for this tutorial, we provide in the link from the “Getting started” section to download the example image. This image is in the .tif format and will be manipulated further in the section n°3.\n\n\nShow the code\nimport os\nimport yaml\nimport counting_wh.wh_utils.planet_utils\n\n# Define the path to your zip file\nzip_path = \"images/raw_images\" \n#AFUN: should this path be training/raw_images? \n#If so, need to be careful where we download the image zip file. Might need to adaot this. \n\n# Run extraction\ncounting_wh.wh_utils.planet_utils.extract_zip(zip_path)\n\n\n\n\n3.\nWe will now transform the example raw tif image into a usable png for future steps. It creates a padded png image to exactly match the size dividable by the stride and tile size.\nCaution that you need to define the config file called “config_train_Drive_UN” as instructed to make sure it matches your paths and runs everything smoothly.\n\n\nShow the code\nimport os\nimport yaml\nimport counting_wh.train\n\n#Run preparation of the tif files into png and renamed the tif. \ncounting_wh.train.prepare(\"config_train_Drive_UN.yaml\")\n \n\n\nDoing gdal work...\nDone with gdal work for 20240101_mimal_test.tif\nNew Width:  14976 New Height:  12064\nProcessed 1/1 images\n\n\n\n\n4.\nOnce the .png image is created, as we are training the model, we need to label the training image. In order to do so, you need to use LabelMe.\nLabelMe is called in your terminal, manually typing “labelme”.\nXXXXXXX insert image of label me opening, to make sure they can open it via the terminal?\nYou have to then annotate the waterholes on your padded png image. This creates at the end a .json file with all my bounding boxes definitions. Those labels will be needed in the next step to segment the image and the corresponding labels for training purposes. XXXXXXXX insert gif of manual label me annotation?\n\n\n5.\nOnce the whole manual annotation is done, save the outputs, and come back to this script to run the segmentation of the padded png image you just labelled.\n\n\nShow the code\nimport os\nimport yaml\nimport counting_wh.train\n\n#segment the png images\ncounting_wh.train.segment(\"config_train_Drive_UN.yaml\", train_val_split=0.8)\n\n\n\n\n6.\nAfter the segmentation, we evaluate the result of the segmentation and production of material to train the model using the “train.describe” function. Run the bellow cell to describe the results of segmented images.\n\n\nShow the code\nimport sys\nimport os\nimport yaml\nimport counting_wh.train\n\n#describe the created segmented images: \ncounting_wh.train.describe(\"config_train_Drive_UN.yaml\")\n\n\nConfig path: D:\\Waterholes_project\\counting_waterholes\\training_v4\\output\\images\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[2], line 8\n      5 import counting_boats.train\n      7 #describe the created segmented images: \n----&gt; 8 counting_boats.train.describe(\"config_train_Drive.yaml\")\n\nFile c:\\Users\\fossatia\\Documents\\Waterholes_project\\counting_waterholes\\counting_boats\\train.py:216, in describe(config)\n    213 lab_path = os.path.join(labdir, lab)\n    215 # Check if file is empty first (file size = 0)\n--&gt; 216 if os.path.getsize(lab_path) == 0:\n    217     num_tiles_no_labels += 1\n    218     continue\n\nFile c:\\Users\\fossatia\\AppData\\Local\\miniconda3\\envs\\Boats\\lib\\genericpath.py:50, in getsize(filename)\n     48 def getsize(filename):\n     49     \"\"\"Return the size of a file, reported by os.stat().\"\"\"\n---&gt; 50     return os.stat(filename).st_size\n\nKeyboardInterrupt: \n\n\n\n\n\n7.\nBefore proceeding to the training of the model, we need to apply the cull command which will remove images with no labels until 10% of the training set has no labels. This is a recommended procedure from Yolo to maintain efficient model training parameters. This has to be done post segmentation as we don’t know prior the the amount.\n\n\nShow the code\nimport sys\nimport os\nimport yaml\nimport random\nimport shutil\nfrom pathlib import Path \nimport counting_wh.train\n\n#describe the created segmented images: \ncounting_wh.train.cull_AF(\"config_train_Drive_UN.yaml\")\n\n\nAnalyzing label files in: D:\\Waterholes_project\\counting_waterholes\\training_v4\\output\\labels\\train\nLooking for corresponding images in: D:\\Waterholes_project\\counting_waterholes\\training_v4\\output\\images\\train\nTotal label files found: 13565\nEmpty label files found: 6063\nNon-empty label files: 7502\nMoving 5230 empty label files to maintain 10% ratio\n\n--- SUMMARY ---\nTotal label files moved: 5230\nTotal image files moved: 5230\nRemaining total label files: 8335\nRemaining empty label files: 833\nEmpty labels now make up 9.99% of the dataset\nEmpty labels moved to: D:\\Waterholes_project\\counting_waterholes\\training_v4\\output\\labels\\moved_empty_labels\nCorresponding images moved to: D:\\Waterholes_project\\counting_waterholes\\training_v4\\output\\images\\moved_empty_images\n\nSUCCESS: Empty labels now make up 10% or less of the dataset.\n\n\n\n\n8.\nNow that we have less than 10% of the images unlabelled, we can train the model, but first let’s reorganise the folders to be properly used in the model.\n\n\nShow the code\nimport sys\nimport os\nimport yaml\nimport counting_wh.train\n\ncounting_wh.train.reorganize_folders(\"config_train_Drive_UN.yaml\")\n\n\nCopying from D:\\Waterholes_project\\counting_waterholes\\training_v4\\output\\images\\val to D:\\Waterholes_project\\counting_waterholes\\training_v4\\train\\val\\images\nSuccessfully copied to D:\\Waterholes_project\\counting_waterholes\\training_v4\\train\\val\\images\nCopying from D:\\Waterholes_project\\counting_waterholes\\training_v4\\output\\images\\train to D:\\Waterholes_project\\counting_waterholes\\training_v4\\train\\train\\images\nSuccessfully copied to D:\\Waterholes_project\\counting_waterholes\\training_v4\\train\\train\\images\nCopying from D:\\Waterholes_project\\counting_waterholes\\training_v4\\output\\labels\\val to D:\\Waterholes_project\\counting_waterholes\\training_v4\\train\\val\\labels\nSuccessfully copied to D:\\Waterholes_project\\counting_waterholes\\training_v4\\train\\val\\labels\nCopying from D:\\Waterholes_project\\counting_waterholes\\training_v4\\output\\labels\\train to D:\\Waterholes_project\\counting_waterholes\\training_v4\\train\\train\\labels\nSuccessfully copied to D:\\Waterholes_project\\counting_waterholes\\training_v4\\train\\train\\labels\n\n----- REORGANIZATION SUMMARY -----\nSuccessful operations: 4\nFailed operations: 0\n\nAll folders were successfully reorganized!\n\nNew structure:\n- D:\\Waterholes_project\\counting_waterholes\\training_v4\\train/val/images (copied from D:\\Waterholes_project\\counting_waterholes\\training_v4\\output/images/val)\n- D:\\Waterholes_project\\counting_waterholes\\training_v4\\train/val/labels (copied from D:\\Waterholes_project\\counting_waterholes\\training_v4\\output/labels/val)\n- D:\\Waterholes_project\\counting_waterholes\\training_v4\\train/train/images (copied from D:\\Waterholes_project\\counting_waterholes\\training_v4\\output/images/train)\n- D:\\Waterholes_project\\counting_waterholes\\training_v4\\train/train/labels (copied from D:\\Waterholes_project\\counting_waterholes\\training_v4\\output/labels/train)\n\n\n\n\n9.\nThe following code provides you with the line of code to run in yolo in order to train a neural network model. Before doing so, modifiy the directory of the yolo model training path in the config file called “config_train_Drive_UN.yaml”.\nThen, run this code which provides you with the command to excecute in the cmd terminal of the Yolov5.\nXXXXXXXXXXXXXXX insert screenshot of the yoloy terminal and what it looks like when it’s training? Maybe a screen shot and a gif when it trains?\n\n\nShow the code\nimport sys\nimport os\nimport yaml\nimport counting_wh.train\n\n#describe the created segmented images: \ncounting_wh.train.train(\"config_train_Drive_UN.yaml\")\n\n\n\npython C:/Users/fossatia/Documents/Waterholes_project/yolov5/train.py --device cuda:0 --img 416 --batch 8 --workers 6 --epochs 100 --data config_train_Drive.yaml --weights C:/Users/fossatia/Documents/Waterholes_project/yolov5/runs/train/exp3/weights/best.pt --save-period 50\n\n\n\n\nComment from AF (08.09): Need to run it and see what it actually prints. Because I ran with the line bellow to train. So need to make sure what we advise to the user as well in term of batch size, img, workers epochs etc…\n\n\nShow the code\npython train.py --workers 2 --img 416 --batch 8 --epochs 150 --data config_train_Drive_UN.yaml --weights yolov5s.pt --cache disk\n\n\nSo it actually seems that it runs automatically with the function train but we do not see any progression… Prefer for now to run it manually in the cmd of the yolov5 folder.\nChanged the cache to the SSD drive we are using as we are limited in the storage available locally. Needed to set the project to the SSD which saves the outputs and doesn’t increase the C: storage usage.\nNeed to actually create the D:/temp and D:/yolo_run on your Drive or external directory\n\n\nShow the code\nset TMPDIR=D:/temp\nset TEMP=D:/temp\nset TMP=D:/temp\nset KMP_DUPLICATE_LIB_OK=TRUE \npython C:/Users/fossatia/Documents/Waterholes_project/yolov5/train.py --device cuda:0 --img 416 --batch 4 --workers 2 --epochs 50 --data C:\\Users\\fossatia\\Documents\\Waterholes_project\\counting_waterholes\\config_train_Drive.yaml --weights C:/Users/fossatia/Documents/Waterholes_project/yolov5/runs/train/exp3/weights/best.pt --cache False --project D:/yolo_runs\n\n\nThe set KMP_DUPLICATE_LIB_OK=TRUE is not recommended on the error command… I tried to google it and it seems we should force an install of the Nomkl using ‘conda install nomkl –channel conda-forge’. However, by doing so, dependencies might be altered. To be checked.\nEnd of this script."
  },
  {
    "objectID": "post_training_UN.html",
    "href": "post_training_UN.html",
    "title": "Testing the trained network and output generation",
    "section": "",
    "text": "Notebook to proceed with the following steps once the yolo model has been trained. Will run manually here sequences of the classify.py and testing.py scripts.\nIdeally, the runing of this section can be done by calling modes that run pipelines out of Charlie’s code. Let’s see how much of those automatisations can be used here. Here is his comment from the testing.py file:\nUtility functions for training/validation pipeline.\nIncludes:\n- prepare: Prepare the images for segmentation\n- segment: Segment the images\n- run_detection: Run the YoloV5 detection\n- backwards_annotation: Generate labelme style annotations from the classifications\n- compare_detections_to_ground_truth: Match up labels and detections, compare them, and save the results\n- confusion_matrix: Summarize the results of the comparison\nLoad first all the configs and required packs:\n\n\nShow the code\n\nimport os\nimport shutil\nimport yaml \nimport argparse\nimport os.path as path\nimport scipy.cluster\nimport scipy.spatial\nimport json\nimport sys\nimport subprocess\n\nimport numpy as np\nimport pandas as pd\nimport scipy\nimport random\nfrom PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nfrom tqdm import tqdm\nimport torch\nimport stat\nimport shutil\nfrom datetime import datetime\n\nfrom counting_wh.wh_utils.config import cfg\nfrom counting_wh.wh_utils import image_cutting_support as ics\nfrom counting_wh.wh_utils import heatmap as hm\n\nimport counting_wh.wh_utils.classifier\n# cluster, process_clusters, read_classifications, pixel2latlong\n\n# Add the project root to sys.path (adjust as needed)\nsys.path.append(os.path.abspath(\"counting_waterholes\"))\n\n\nc:\\Users\\fossatia\\AppData\\Local\\miniconda3\\envs\\Boats\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nYOLOv5  v7.0-394-g86fd1ab2 Python-3.10.16 torch-1.12.1+cu113 CUDA:0 (GeForce GTX 1080, 8192MiB)\n\nFusing layers... \nModel summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\nAdding AutoShape... \n\n\nThen we can select manually which function we want to run. First, from the testing aoi tif file, we need to prepare the padded png image using the testing.prepare():\n\n\nShow the code\n# Run preparation:\nimport counting_wh.wh_utils.testing\n\n\ncounting_wh.wh_utils.testing.prepare(\"counting_waterholes/testing\", \"config_test_GPU.yaml\")\n\n\nThen I need to use the created png to label it with labelme. This will allow us to compare my annotation to the detection of the trained model i.e. test the model.\nOnce the manual annotation is done, we can apply the segmentation used from the testing.segment():\n\n\nShow the code\n#run segmentation without spliting 80% of the images for validation!\nimport counting_wh.wh_utils.testing\n\ncounting_wh.wh_utils.testing.segment(r\"D:/Waterholes_project/counting_waterholes/testing\", \"config_test_Drive.yaml\")\n\n\nCropping Image: D:/Waterholes_project/counting_waterholes/testing\\./pngs\\20240204_mimal_test.png\n[12064 14976     3]\nWe will have:  15933  images maximum\n0% of images without labels will be removed\n\n\nSaving Segments: 100%|██████████| 15933/15933 [05:33&lt;00:00, 47.77it/s] \n\n\nSkipped 728 images\nEmpty 0 images\nCropping Image: D:/Waterholes_project/counting_waterholes/testing\\./pngs\\20240324_mimal_test.png\n[12064 14976     3]\nWe will have:  15933  images maximum\n0% of images without labels will be removed\n\n\nSaving Segments: 100%|██████████| 15933/15933 [08:10&lt;00:00, 32.46it/s] \n\n\nSkipped 1676 images\nEmpty 0 images\nCropping Image: D:/Waterholes_project/counting_waterholes/testing\\./pngs\\20240415_mimal_test.png\n[12064 14976     3]\nWe will have:  15933  images maximum\n0% of images without labels will be removed\n\n\nSaving Segments: 100%|██████████| 15933/15933 [11:48&lt;00:00, 22.49it/s] \n\n\nSkipped 790 images\nEmpty 0 images\nSegregating by day...\n01_01_2024\n04_02_2024\n24_03_2024\n15_04_2024\nSegregating by image...\nSegregating by image...\nSegregating by image...\nSegregating by image...\nSegregating by day...\n01_01_2024\n04_02_2024\n24_03_2024\n15_04_2024\nSegregating by image...\nSegregating by image...\nSegregating by image...\nSegregating by image...\n\n\nUsing those segmented labelled images, we can run the detection of waterholes using the trained model, and compare my label with the detection of the model.\nDebugging section to recognise and work well with the GPU:\nNote to user: Need to update the torchvision to match the cuda (GPU) version. using the ‘nvidia-smi’ command, you get the cuda version (my case: 11) so I need to get a version of torch and torchaudio with to 11.xx. Need to ‘pip uninstall torch torchvision’, and then install the correct version, in my case: ‘pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 –extra-index-url https://download.pytorch.org/whl/cu113’\nOther version to be found on this website: https://pytorch.org/get-started/previous-versions/\nThe testing.segment function and run_detection work to use the segmented images folders grouped per date. left as it is for now but just something to bear in mind!\n\n\nShow the code\n#check of the GPU found or not?\ntorch.cuda.is_available()\n\n\nTrue\n\n\n\n\nShow the code\n#run detection on my testing \nimport counting_wh.wh_utils.testing\n\ncounting_wh.wh_utils.testing.run_detection(r\"D:/Waterholes_project/counting_waterholes/testing_v3\", \"config_test_Drive.yaml\")\n\n\nCopy of the order command that is run run to yolo cmd by the function run_detectionpage. Just in order to debug and be informative. Do not run.\n\n\nShow the code\npython C:/Users/fossatia/Documents/Waterholes_project/yolov5/detect.py --imgsz 416 --save-txt --save-conf --weights C:/Users/fossatia/Documents/Waterholes_project/yolov5/runs/train/exp3/weights/best.pt --source D:\\\\Waterholes_project\\\\counting_waterholes\\\\testing\\\\segmented_images\\\\04_02_2024\\\\20240204_mimal_test --device cuda:0 --nosave --conf-thres 0.15\n\n\nUse the detection output of the model on my testing images to produce labelme style annotation using the backwards_annotation():\n\n\nShow the code\n#run the annotation of the images using the detection of the model:\nimport counting_wh.wh_utils.testing\n\ncounting_wh.wh_utils.testing.backwards_annotation_AF(r\"D:/Waterholes_project/counting_waterholes/testing_v3\", \"config_test_Drive.yaml\")\n\n\nThen finally, compare the detected WH with my labeled WH:\n\n\nShow the code\n#comparison of my labels with the detected WH:\nimport counting_wh.wh_utils.testing\n\ncounting_wh.wh_utils.testing.compare_detections_to_ground_truth(r\"D:/Waterholes_project/counting_waterholes/testing_v3\", \"config_test_Drive.yaml\")\n\n\nLabel directory D:/Waterholes_project/counting_waterholes/testing_v3\\./labels\\05_02_2025\\050225_NT does not exist, skipping image...\nraw_images folder D:/Waterholes_project/counting_waterholes/testing_v3\\./raw_images\nCould not parse date from 050225_NT.csv\n\n\nCreate the confusion matrix which summarises the results:\n\n\nShow the code\n#create the confusion matrix\nimport counting_wh.wh_utils.testing\n\ncounting_wh.wh_utils.testing.confusion_matrix_AF(r\"D:/Waterholes_project/counting_waterholes/testing_v3\", \"config_test_Drive.yaml\")\n\n\nPossible to process a single images by comparing the detections and labels for a single image, used in a function but not usefull by hand.\n\n\nShow the code\n# #create the confusion matrix\n# import counting_wh.wh_utils.testing\n\n# counting_wh.wh_utils.testing.process_image_AF(r\"D:\\Waterholes_project\\counting_waterholes\\testing_v3\\classifications\", r\"D:/Waterholes_project/counting_waterholes/testing_v3/labels\", \"config_test_Drive.yaml\")\n\n\nCompare the counts one by one:\n\n\nShow the code\n#comparison labelled vs detected WH:\nimport counting_wh.wh_utils.testing\n\ncounting_wh.wh_utils.testing.waterholes_count_compare(r\"D:/Waterholes_project/counting_waterholes/testing_v3\", \"config_test_Drive.yaml\")\n\n\nNow we want to plot the waterholes on the images. To do that, I need first to stich the training images together:\n\n\nShow the code\nimport os\nimport datetime\nfrom PIL import Image\nimport counting_wh.wh_utils.stitch_PNGs\n\ncounting_wh.wh_utils.stitch_PNGs.stitch(r\"D:\\Waterholes_project\\counting_waterholes\\testing_v3\\stitching\")\n\n\nMax x: 92, Max y: 60\nSaved stitched image to D:\\Waterholes_project\\counting_waterholes\\testing_v3\\stitching\\stitched.png\n\n\nNow I want to plot the waterholes, but the function is made for boats so I need to modify it to work on WH. WIP\n\n\nShow the code\n#plot WH using that stiched image:\nimport counting_wh.wh_utils.testing\n\ncounting_wh.wh_utils.testing.plot_waterholes(r\"D:/Waterholes_project/counting_waterholes/testing_v3\", \"config_test_Drive.yaml\")"
  }
]